# Project Presentation Outline
## Employee Attrition Prediction System with Explainable AI

---

## Slide 1: Title
**Employee Attrition Prediction with Explainable AI and LDA Topic Modeling**
- Your Name
- Guide Name
- Institution
- Date

---

## Slide 2: Problem Statement
**Why Employee Attrition Matters**

- 4.5 million employees leave jobs monthly (US alone)
- Replacement cost: 50-200% of annual salary
- Hidden costs: Knowledge loss, productivity decline, team disruption
- **Solution needed**: Predict WHO will leave and WHY

---

## Slide 3: Research Objectives

1. Build a prediction system that works with multiple dataset formats
2. Achieve high accuracy (target: >90%)
3. Provide explainable results (not black-box)
4. Generate actionable recommendations
5. Deploy as user-friendly web application

---

## Slide 4: System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         PRESENTATION LAYER              â”‚
â”‚    React + TypeScript + Plotly.js       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         BUSINESS LOGIC LAYER            â”‚
â”‚   ML Pipeline (Risk Calculation,        â”‚
â”‚   SHAP Analysis, LDA Topics)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         DATA PERSISTENCE LAYER          â”‚
â”‚         PostgreSQL Database             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Slide 5: Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| Frontend | React 18 | User Interface |
| Language | TypeScript | Type Safety |
| Styling | Tailwind CSS | Responsive Design |
| Charts | Plotly.js | Interactive Visualizations |
| Database | PostgreSQL | Data Storage |
| State | TanStack Query | Caching & Sync |

---

## Slide 6: Dataset Flexibility (Key Innovation)

**Problem**: Different organizations use different HR data formats

**Solution**: Automatic format detection

| Dataset Type | Key Columns | Source |
|--------------|-------------|--------|
| IBM HR | Attrition, MonthlyIncome, OverTime | IBM Research |
| Kaggle HR | left, satisfaction_level, salary | Kaggle Competition |
| AmbitionBox | Likes, Dislikes, Overall_rating | Employee Reviews |

**How it works**: Column signature matching algorithm

---

## Slide 7: Algorithm 1 - Risk Score Calculation

**Based on research findings:**

```
Risk Score = Base (0.3) + Î£(Factor Weights)

Factors:
â”œâ”€â”€ OverTime = Yes         â†’ +0.15
â”œâ”€â”€ Job Satisfaction < 2   â†’ +0.12
â”œâ”€â”€ Work-Life Balance < 2  â†’ +0.10
â”œâ”€â”€ Years at Company < 2   â†’ +0.08
â”œâ”€â”€ Income < Median        â†’ +0.07
â””â”€â”€ Distance > 20km        â†’ +0.05
```

**Output**: Score between 0 and 1

---

## Slide 8: Algorithm 2 - SHAP Explainability

**What is SHAP?**
- SHapley Additive exPlanations
- Based on game theory (Shapley Values)
- Answers: "How much did each feature contribute?"

**Formula:**
```
Ï†áµ¢ = Î£ [f(S âˆª {i}) - f(S)] Ã— weight
```

**Output**: Feature importance with direction (increases/decreases risk)

---

## Slide 9: Algorithm 3 - LDA Topic Modeling

**What is LDA?**
- Latent Dirichlet Allocation
- Discovers hidden topics in text
- Used for: Employee review analysis

**Process:**
1. Input: Employee review text (Likes, Dislikes)
2. Process: Find common themes
3. Output: Topics with sentiment (positive/negative)

**Example Topics:**
- Work Culture (positive)
- Management Issues (negative)
- Career Growth (neutral)

---

## Slide 10: Five-Level Risk Classification

| Risk Level | Score Range | Action Required |
|------------|-------------|-----------------|
| Very Low | 0.00 - 0.20 | Monitor |
| Low | 0.20 - 0.40 | Standard engagement |
| Medium | 0.40 - 0.60 | Proactive check-in |
| High | 0.60 - 0.80 | Intervention needed |
| Critical | 0.80 - 1.00 | Immediate action |

---

## Slide 11: Data Flow Pipeline

```
CSV Upload â†’ Parse â†’ Detect Type â†’ Validate
                                      â”‚
                                      â–¼
Dashboard â† Visualize â† Store â† Analyze
```

**5 Steps:**
1. **Ingestion**: User uploads CSV
2. **Detection**: Identify dataset format
3. **Analysis**: Calculate risk scores
4. **Explanation**: Generate SHAP + LDA
5. **Visualization**: Interactive charts

---

## Slide 12: Visualization Components

1. **Risk Distribution** - Pie/Donut chart showing employee risk levels
2. **Department Analysis** - Bar chart comparing departments
3. **Feature Importance** - Horizontal bars with color coding
4. **Topic Analysis** - Sentiment-colored bar chart
5. **Trend Analysis** - Line chart for predictions

---

## Slide 13: Recommendation Engine

**Rule-based system:**

```
IF OverTime importance > 10%
   â†’ Recommend: "Implement workload balancing"

IF Salary below median
   â†’ Recommend: "Conduct market salary analysis"

IF Negative management topics > 20%
   â†’ Recommend: "Leadership training program"
```

---

## Slide 14: Database Design

**Two main tables:**

```
datasets                    analysis_results
â”œâ”€â”€ id (UUID)              â”œâ”€â”€ id (UUID)
â”œâ”€â”€ name                   â”œâ”€â”€ dataset_id (FK)
â”œâ”€â”€ raw_data (JSON)        â”œâ”€â”€ predictions (JSON)
â”œâ”€â”€ column_names[]         â”œâ”€â”€ feature_importance (JSON)
â”œâ”€â”€ row_count              â”œâ”€â”€ topics (JSON)
â””â”€â”€ created_at             â””â”€â”€ recommendations (JSON)
```

---

## Slide 15: Demo Screenshots

*(Add actual screenshots of your application)*

1. Dashboard with statistics
2. Dataset upload interface
3. Analysis running
4. Results with charts
5. Recommendations panel

---

## Slide 16: Results & Accuracy

**Based on research paper:**
- Transformer Model: 96.95% accuracy
- Random Forest: 83.26% accuracy
- XGBoost: 82.81% accuracy
- Logistic Regression: 81% accuracy

**Expected Impact:**
- 25-30% attrition reduction with targeted interventions

---

## Slide 17: Key Contributions

1. **Multi-format support** - Works with any HR dataset
2. **Dual explainability** - SHAP (quantitative) + LDA (qualitative)
3. **Five-level risk** - Beyond binary yes/no predictions
4. **Web deployment** - Production-ready application
5. **Actionable insights** - Not just predictions, but recommendations

---

## Slide 18: Limitations & Future Work

**Current Limitations:**
- Mock ML pipeline (simulated predictions)
- No real-time data integration
- Single organization at a time

**Future Enhancements:**
- Real Python ML backend
- HRIS system integration
- Continuous model retraining
- Multi-organization comparison

---

## Slide 19: Conclusion

âœ… Built production-ready attrition prediction system
âœ… Supports multiple dataset formats automatically
âœ… Provides explainable AI through SHAP and LDA
âœ… Generates actionable retention recommendations
âœ… Deployed as responsive web application

**Key Takeaway**: Combining prediction with explanation enables better decision-making

---

## Slide 20: Q&A

**Thank You**

Questions?

---

## Appendix: Technical Terms

| Term | Simple Explanation |
|------|-------------------|
| SHAP | Method to explain which factors matter most |
| LDA | Algorithm to find topics in text |
| Transformer | Advanced neural network architecture |
| TypeScript | JavaScript with type checking |
| PostgreSQL | Database for storing data |
| React | Library for building user interfaces |

---

## Appendix: Code Structure

```
src/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ dashboard/      â†’ Statistics cards
â”‚   â”œâ”€â”€ charts/         â†’ Plotly visualizations
â”‚   â”œâ”€â”€ datasets/       â†’ Upload & list
â”‚   â””â”€â”€ analysis/       â†’ Results view
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ useDatasets.ts  â†’ Data fetching
â”‚   â””â”€â”€ useAnalysis.ts  â†’ Analysis operations
â”œâ”€â”€ lib/
â”‚   â””â”€â”€ mockAnalysis.ts â†’ ML simulation
â””â”€â”€ types/
    â””â”€â”€ dataset.ts      â†’ Type definitions
```

---

## Appendix: SHAP Calculation Process (Detailed Diagram)

<presentation-mermaid>
flowchart TD
    subgraph INPUT["ğŸ“¥ Input Phase"]
        A[Employee Data Record] --> B[Feature Vector]
        B --> C["Features: OverTime, Salary, Satisfaction, etc."]
    end
    
    subgraph BASELINE["ğŸ“Š Baseline Calculation"]
        D[Calculate Base Prediction] --> E["E[f(x)] = Average prediction\nacross all employees"]
        E --> F["Baseline Risk = 0.35\n(example)"]
    end
    
    subgraph COALITIONS["ğŸ¯ Coalition Formation"]
        G[Generate Feature Subsets] --> H["2^n possible combinations"]
        H --> I["S = {empty}"]
        H --> J["S = {OverTime}"]
        H --> K["S = {OverTime, Salary}"]
        H --> L["S = {all features}"]
    end
    
    subgraph MARGINAL["âš–ï¸ Marginal Contribution"]
        M["For each feature i:"] --> N["Calculate f(S âˆª {i}) - f(S)"]
        N --> O["OverTime: 0.52 - 0.35 = +0.17"]
        N --> P["Salary: 0.42 - 0.35 = +0.07"]
        N --> Q["Satisfaction: 0.30 - 0.35 = -0.05"]
    end
    
    subgraph SHAPLEY["ğŸ”¢ Shapley Value Formula"]
        R["Ï†áµ¢ = Î£ |S|!(n-|S|-1)!/n! Ã— [f(Sâˆª{i}) - f(S)]"]
        R --> S["Weight = Coalition probability"]
        S --> T["Sum over all coalitions"]
    end
    
    subgraph OUTPUT["ğŸ“¤ Output"]
        U[SHAP Values per Feature] --> V["OverTime: +0.15\nSalary: +0.08\nSatisfaction: -0.05\nDistance: +0.03"]
        V --> W[Feature Importance Ranking]
        W --> X["1. OverTime (â†‘ risk)\n2. Salary (â†‘ risk)\n3. Satisfaction (â†“ risk)"]
    end
    
    INPUT --> BASELINE
    BASELINE --> COALITIONS
    COALITIONS --> MARGINAL
    MARGINAL --> SHAPLEY
    SHAPLEY --> OUTPUT
    
    style INPUT fill:#e0f2fe,stroke:#0284c7
    style BASELINE fill:#fef3c7,stroke:#d97706
    style COALITIONS fill:#f3e8ff,stroke:#9333ea
    style MARGINAL fill:#dcfce7,stroke:#16a34a
    style SHAPLEY fill:#fee2e2,stroke:#dc2626
    style OUTPUT fill:#f0fdf4,stroke:#22c55e
</presentation-mermaid>

### SHAP Step-by-Step Explanation:

1. **Input**: Single employee record with all features
2. **Baseline**: Calculate average prediction across dataset (expected value)
3. **Coalitions**: Generate all possible feature combinations (2^n subsets)
4. **Marginal Contribution**: For each feature, measure how adding it changes the prediction
5. **Shapley Formula**: Weight contributions by coalition probability
6. **Output**: SHAP value for each feature (positive = increases risk, negative = decreases risk)

---

## Appendix: LDA Topic Extraction Process (Detailed Diagram)

<presentation-mermaid>
flowchart TD
    subgraph CORPUS["ğŸ“š Document Collection"]
        A1["Review 1: 'Great work culture and supportive team'"]
        A2["Review 2: 'Poor management and long hours'"]
        A3["Review 3: 'Good salary but no growth'"]
        A4["Review N: ..."]
    end
    
    subgraph PREPROCESS["ğŸ”§ Preprocessing"]
        B1[Tokenization] --> B2["'great', 'work', 'culture', 'supportive', 'team'"]
        B2 --> B3[Remove Stopwords]
        B3 --> B4["'great', 'work', 'culture', 'supportive', 'team'"]
        B4 --> B5[Stemming/Lemmatization]
        B5 --> B6["'great', 'work', 'cultur', 'support', 'team'"]
    end
    
    subgraph INIT["ğŸ² Initialization"]
        C1["Set K = number of topics\n(e.g., K=5)"] --> C2["Random topic assignment\nfor each word"]
        C2 --> C3["Word 'culture' â†’ Topic 2\nWord 'salary' â†’ Topic 4\nWord 'manager' â†’ Topic 1"]
    end
    
    subgraph DIRICHLET["ğŸ“ Dirichlet Distributions"]
        D1["Î± = Document-Topic prior"] --> D2["Î¸d ~ Dirichlet(Î±)\n= Topic mixture for doc d"]
        D3["Î² = Topic-Word prior"] --> D4["Ï†k ~ Dirichlet(Î²)\n= Word distribution for topic k"]
    end
    
    subgraph GIBBS["ğŸ”„ Gibbs Sampling Iteration"]
        E1["For each word w in document d:"] --> E2["Remove w from current topic"]
        E2 --> E3["Calculate P(topic k | rest)"]
        E3 --> E4["P(k|d,w) âˆ (n_dk + Î±) Ã— (n_kw + Î²)/(n_k + VÎ²)"]
        E4 --> E5["Sample new topic\nfrom distribution"]
        E5 --> E6["Assign w to new topic"]
        E6 --> E1
    end
    
    subgraph CONVERGE["âœ… Convergence Check"]
        F1["Repeat Gibbs sampling\n1000+ iterations"] --> F2{"Topics\nstabilized?"}
        F2 -->|No| E1
        F2 -->|Yes| F3[Extract final topics]
    end
    
    subgraph TOPICS["ğŸ·ï¸ Extracted Topics"]
        G1["Topic 1: WORK CULTURE\n'culture', 'team', 'environment', 'support'"]
        G2["Topic 2: MANAGEMENT\n'manager', 'leadership', 'boss', 'supervisor'"]
        G3["Topic 3: COMPENSATION\n'salary', 'pay', 'benefits', 'bonus'"]
        G4["Topic 4: GROWTH\n'career', 'promotion', 'learning', 'training'"]
        G5["Topic 5: WORK-LIFE\n'hours', 'balance', 'overtime', 'flexible'"]
    end
    
    subgraph SENTIMENT["ğŸ˜ŠğŸ˜ğŸ˜  Sentiment Analysis"]
        H1[Match words to sentiment lexicon] --> H2["'great', 'supportive' â†’ Positive\n'poor', 'long' â†’ Negative"]
        H2 --> H3["Topic Sentiment Score:\nWork Culture: +0.7 (Positive)\nManagement: -0.4 (Negative)"]
    end
    
    CORPUS --> PREPROCESS
    PREPROCESS --> INIT
    INIT --> DIRICHLET
    DIRICHLET --> GIBBS
    GIBBS --> CONVERGE
    CONVERGE --> TOPICS
    TOPICS --> SENTIMENT
    
    style CORPUS fill:#e0f2fe,stroke:#0284c7
    style PREPROCESS fill:#fef3c7,stroke:#d97706
    style INIT fill:#f3e8ff,stroke:#9333ea
    style DIRICHLET fill:#fce7f3,stroke:#db2777
    style GIBBS fill:#dcfce7,stroke:#16a34a
    style CONVERGE fill:#fed7aa,stroke:#ea580c
    style TOPICS fill:#dbeafe,stroke:#2563eb
    style SENTIMENT fill:#f0fdf4,stroke:#22c55e
</presentation-mermaid>

### LDA Step-by-Step Explanation:

1. **Document Collection**: Gather all employee reviews (Likes, Dislikes columns)
2. **Preprocessing**: Clean text - tokenize, remove stopwords, stem words
3. **Initialization**: Randomly assign each word to one of K topics
4. **Dirichlet Priors**: Set hyperparameters Î± (document-topic) and Î² (topic-word)
5. **Gibbs Sampling**: Iteratively reassign words to topics based on probability
6. **Convergence**: Repeat until topic assignments stabilize
7. **Topic Extraction**: Identify top words per topic, name topics
8. **Sentiment**: Classify each topic as positive/negative/neutral

---

## Appendix: Complete System Flow Diagram

<presentation-mermaid>
flowchart LR
    subgraph USER["ğŸ‘¤ User Interface"]
        U1[Upload CSV] --> U2[View Dashboard]
        U2 --> U3[Analyze Data]
        U3 --> U4[Review Results]
    end
    
    subgraph FRONTEND["âš›ï¸ React Frontend"]
        F1[DatasetUpload.tsx] --> F2[useDatasets Hook]
        F3[AnalysisView.tsx] --> F4[useAnalysis Hook]
        F5[Charts Components] --> F6[Plotly.js Render]
    end
    
    subgraph ENGINE["ğŸ§  Analysis Engine"]
        E1["detectDatasetType()"] --> E2["processEmployeeData()"]
        E2 --> E3["calculateRiskScore()"]
        E3 --> E4["calculateSHAP()"]
        E4 --> E5["extractTopics()"]
        E5 --> E6["generateRecommendations()"]
    end
    
    subgraph DATABASE["ğŸ—„ï¸ PostgreSQL"]
        D1[(datasets table)] --> D2[(analysis_results table)]
    end
    
    USER --> FRONTEND
    FRONTEND --> ENGINE
    ENGINE --> DATABASE
    DATABASE --> FRONTEND
    
    style USER fill:#e0f2fe,stroke:#0284c7
    style FRONTEND fill:#dcfce7,stroke:#16a34a
    style ENGINE fill:#fef3c7,stroke:#d97706
    style DATABASE fill:#f3e8ff,stroke:#9333ea
</presentation-mermaid>

---

## Appendix: Risk Score Algorithm Flowchart

<presentation-mermaid>
flowchart TD
    START([Start]) --> INPUT[/Input: Employee Record/]
    INPUT --> BASE["baseRisk = 0.30"]
    
    BASE --> OT{"OverTime\n= 'Yes'?"}
    OT -->|Yes| OT_ADD["risk += 0.15"]
    OT -->|No| SAT
    OT_ADD --> SAT
    
    SAT{"JobSatisfaction\n< 2?"}
    SAT -->|Yes| SAT_ADD["risk += 0.12"]
    SAT -->|No| WLB
    SAT_ADD --> WLB
    
    WLB{"WorkLifeBalance\n< 2?"}
    WLB -->|Yes| WLB_ADD["risk += 0.10"]
    WLB -->|No| YRS
    WLB_ADD --> YRS
    
    YRS{"YearsAtCompany\n< 2?"}
    YRS -->|Yes| YRS_ADD["risk += 0.08"]
    YRS -->|No| INC
    YRS_ADD --> INC
    
    INC{"MonthlyIncome\n< Median?"}
    INC -->|Yes| INC_ADD["risk += 0.07"]
    INC -->|No| DIST
    INC_ADD --> DIST
    
    DIST{"DistanceFromHome\n> 20?"}
    DIST -->|Yes| DIST_ADD["risk += 0.05"]
    DIST -->|No| NOISE
    DIST_ADD --> NOISE
    
    NOISE["Add random noise\nÂ±0.05 (realism)"]
    NOISE --> CLAMP["Clamp to [0, 1]"]
    CLAMP --> CLASSIFY
    
    CLASSIFY{"Classify Risk Level"}
    CLASSIFY --> L1["0.0-0.2: Very Low ğŸŸ¢"]
    CLASSIFY --> L2["0.2-0.4: Low ğŸŸ¡"]
    CLASSIFY --> L3["0.4-0.6: Medium ğŸŸ "]
    CLASSIFY --> L4["0.6-0.8: High ğŸ”´"]
    CLASSIFY --> L5["0.8-1.0: Critical â›”"]
    
    L1 --> OUTPUT[/Output: Risk Score + Level/]
    L2 --> OUTPUT
    L3 --> OUTPUT
    L4 --> OUTPUT
    L5 --> OUTPUT
    OUTPUT --> END([End])
    
    style START fill:#22c55e,stroke:#16a34a
    style END fill:#ef4444,stroke:#dc2626
    style CLASSIFY fill:#3b82f6,stroke:#2563eb
</presentation-mermaid>
