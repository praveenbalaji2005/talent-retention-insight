{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Employee Attrition Prediction System\n",
    "## XAI-Powered Models for Managerial Decision-Making\n",
    "\n",
    "---\n",
    "\n",
    "**Research Basis:** Baydili & Tasci, *\"Predicting Employee Attrition: XAI-Powered Models for Managerial Decision-Making,\"* Systems 2025, 13, 583\n",
    "\n",
    "### What this notebook demonstrates:\n",
    "1. **Transformer Encoder** for tabular HR data prediction\n",
    "2. **SHAP Explainability** ‚Äî why each employee is at risk\n",
    "3. **LDA Topic Modeling** ‚Äî hidden themes in employee reviews\n",
    "4. **Five-Tier Risk Classification** ‚Äî actionable risk levels\n",
    "\n",
    "### How to run:\n",
    "1. Click **Runtime ‚Üí Run all** (or press `Ctrl+F9`)\n",
    "2. To use your own data: upload a CSV in the **\"Upload Your Dataset\"** section\n",
    "3. The system auto-detects IBM HR, Kaggle HR, and AmbitionBox formats\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 0: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (most are pre-installed in Colab)\n",
    "!pip install -q numpy pandas scikit-learn matplotlib seaborn\n",
    "print(\"‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Step 1: Configuration\n",
    "\n",
    "These are the hyperparameters that control the ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Transformer model configuration\"\"\"\n",
    "    d_model: int = 64          # Embedding dimension\n",
    "    n_heads: int = 2           # Number of attention heads\n",
    "    n_layers: int = 3          # Number of transformer layers\n",
    "    dropout: float = 0.1       # Dropout rate\n",
    "    learning_rate: float = 0.001\n",
    "    epochs: int = 100\n",
    "    batch_size: int = 32\n",
    "\n",
    "@dataclass\n",
    "class LDAConfig:\n",
    "    \"\"\"LDA topic modeling configuration\"\"\"\n",
    "    n_topics: int = 5          # Number of topics to discover\n",
    "    n_iterations: int = 100    # Gibbs sampling iterations\n",
    "    alpha: float = 0.1         # Document-topic prior\n",
    "    beta: float = 0.01         # Topic-word prior\n",
    "\n",
    "# Five-tier risk classification thresholds\n",
    "RISK_THRESHOLDS = {\n",
    "    'low': (0.0, 0.20),\n",
    "    'early_warning': (0.20, 0.40),\n",
    "    'moderate': (0.40, 0.60),\n",
    "    'high': (0.60, 0.80),\n",
    "    'critical': (0.80, 1.0)\n",
    "}\n",
    "\n",
    "RISK_COLORS = {\n",
    "    'low': '#22c55e',\n",
    "    'early_warning': '#3b82f6',\n",
    "    'moderate': '#f59e0b',\n",
    "    'high': '#f97316',\n",
    "    'critical': '#ef4444'\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration loaded!\")\n",
    "print(f\"\\nTransformer Config:\")\n",
    "config = ModelConfig()\n",
    "print(f\"  d_model={config.d_model}, n_heads={config.n_heads}, n_layers={config.n_layers}\")\n",
    "print(f\"  epochs={config.epochs}, lr={config.learning_rate}\")\n",
    "print(f\"\\nLDA Config:\")\n",
    "lda_config = LDAConfig()\n",
    "print(f\"  n_topics={lda_config.n_topics}, iterations={lda_config.n_iterations}\")\n",
    "print(f\"  alpha={lda_config.alpha}, beta={lda_config.beta}\")\n",
    "print(f\"\\nRisk Thresholds:\")\n",
    "for level, (low, high) in RISK_THRESHOLDS.items():\n",
    "    print(f\"  {level.upper():15s}: {low:.0%} - {high:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèóÔ∏è Step 2: Transformer Encoder Architecture\n",
    "\n",
    "The Transformer Encoder uses **multi-head self-attention** to learn relationships between features.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Self-Attention**: Each feature \"attends\" to all other features, learning which combinations matter\n",
    "- **Multi-Head**: Multiple parallel attention operations capture different types of relationships\n",
    "- **Residual Connections**: Skip connections that prevent vanishing gradients\n",
    "- **Layer Normalization**: Stabilizes training by normalizing activations\n",
    "\n",
    "### Architecture:\n",
    "```\n",
    "Input ‚Üí Embedding ‚Üí [Attention + FFN] √ó N_layers ‚Üí Sigmoid ‚Üí Probability\n",
    "```\n",
    "\n",
    "### Mathematical Foundation:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder:\n",
    "    \"\"\"\n",
    "    Transformer Encoder for Tabular Data Prediction\n",
    "    \n",
    "    Architecture:\n",
    "    - Multi-head self-attention mechanism\n",
    "    - Position-wise feed-forward networks\n",
    "    - Layer normalization and residual connections\n",
    "    - SELU activation function\n",
    "    \n",
    "    Reference: \"Attention Is All You Need\" (Vaswani et al., 2017)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelConfig = ModelConfig()):\n",
    "        self.config = config\n",
    "        self.weights = {}\n",
    "        self.is_trained = False\n",
    "        \n",
    "    def _initialize_weights(self, input_dim: int):\n",
    "        \"\"\"Initialize transformer weights using Xavier initialization\"\"\"\n",
    "        d_model = self.config.d_model\n",
    "        \n",
    "        # Input embedding\n",
    "        self.weights['W_embed'] = np.random.randn(input_dim, d_model) * np.sqrt(2.0 / input_dim)\n",
    "        self.weights['b_embed'] = np.zeros(d_model)\n",
    "        \n",
    "        # Multi-head attention weights for each layer\n",
    "        for layer in range(self.config.n_layers):\n",
    "            self.weights[f'W_q_{layer}'] = np.random.randn(d_model, d_model) * np.sqrt(2.0 / d_model)\n",
    "            self.weights[f'W_k_{layer}'] = np.random.randn(d_model, d_model) * np.sqrt(2.0 / d_model)\n",
    "            self.weights[f'W_v_{layer}'] = np.random.randn(d_model, d_model) * np.sqrt(2.0 / d_model)\n",
    "            self.weights[f'W_o_{layer}'] = np.random.randn(d_model, d_model) * np.sqrt(2.0 / d_model)\n",
    "            self.weights[f'W_ff1_{layer}'] = np.random.randn(d_model, d_model * 4) * np.sqrt(2.0 / d_model)\n",
    "            self.weights[f'b_ff1_{layer}'] = np.zeros(d_model * 4)\n",
    "            self.weights[f'W_ff2_{layer}'] = np.random.randn(d_model * 4, d_model) * np.sqrt(2.0 / (d_model * 4))\n",
    "            self.weights[f'b_ff2_{layer}'] = np.zeros(d_model)\n",
    "            self.weights[f'gamma_attn_{layer}'] = np.ones(d_model)\n",
    "            self.weights[f'beta_attn_{layer}'] = np.zeros(d_model)\n",
    "            self.weights[f'gamma_ff_{layer}'] = np.ones(d_model)\n",
    "            self.weights[f'beta_ff_{layer}'] = np.zeros(d_model)\n",
    "        \n",
    "        self.weights['W_out'] = np.random.randn(d_model, 1) * np.sqrt(2.0 / d_model)\n",
    "        self.weights['b_out'] = np.zeros(1)\n",
    "        \n",
    "    def _selu(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"SELU activation: Self-Normalizing Exponential Linear Unit\"\"\"\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        return scale * np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "    \n",
    "    def _layer_norm(self, x, gamma, beta, eps=1e-6):\n",
    "        \"\"\"Layer Normalization: normalizes across features\"\"\"\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        std = np.std(x, axis=-1, keepdims=True)\n",
    "        return gamma * (x - mean) / (std + eps) + beta\n",
    "    \n",
    "    def _scaled_dot_product_attention(self, Q, K, V):\n",
    "        \"\"\"Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V\"\"\"\n",
    "        d_k = Q.shape[-1]\n",
    "        scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "        exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
    "        attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n",
    "        return np.dot(attention_weights, V), attention_weights\n",
    "    \n",
    "    def _multi_head_attention(self, x, layer):\n",
    "        \"\"\"Multi-head self-attention mechanism\"\"\"\n",
    "        Q = np.dot(x, self.weights[f'W_q_{layer}'])\n",
    "        K = np.dot(x, self.weights[f'W_k_{layer}'])\n",
    "        V = np.dot(x, self.weights[f'W_v_{layer}'])\n",
    "        attention_output, attention_weights = self._scaled_dot_product_attention(Q, K, V)\n",
    "        output = np.dot(attention_output, self.weights[f'W_o_{layer}'])\n",
    "        return output, attention_weights\n",
    "    \n",
    "    def _feed_forward(self, x, layer):\n",
    "        \"\"\"Position-wise Feed-Forward Network\"\"\"\n",
    "        hidden = np.dot(x, self.weights[f'W_ff1_{layer}']) + self.weights[f'b_ff1_{layer}']\n",
    "        hidden = self._selu(hidden)\n",
    "        output = np.dot(hidden, self.weights[f'W_ff2_{layer}']) + self.weights[f'b_ff2_{layer}']\n",
    "        return output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the complete transformer encoder\"\"\"\n",
    "        hidden = np.dot(x, self.weights['W_embed']) + self.weights['b_embed']\n",
    "        attention_maps = {}\n",
    "        \n",
    "        for layer in range(self.config.n_layers):\n",
    "            # Self-attention with residual\n",
    "            attn_output, attn_weights = self._multi_head_attention(hidden, layer)\n",
    "            hidden = hidden + attn_output\n",
    "            hidden = self._layer_norm(hidden, self.weights[f'gamma_attn_{layer}'], self.weights[f'beta_attn_{layer}'])\n",
    "            attention_maps[f'layer_{layer}'] = attn_weights\n",
    "            \n",
    "            # Feed-forward with residual\n",
    "            ff_output = self._feed_forward(hidden, layer)\n",
    "            hidden = hidden + ff_output\n",
    "            hidden = self._layer_norm(hidden, self.weights[f'gamma_ff_{layer}'], self.weights[f'beta_ff_{layer}'])\n",
    "        \n",
    "        logits = np.dot(hidden, self.weights['W_out']) + self.weights['b_out']\n",
    "        probabilities = 1 / (1 + np.exp(-logits))  # Sigmoid\n",
    "        return probabilities.flatten(), attention_maps\n",
    "    \n",
    "    def fit(self, X, y, verbose=True):\n",
    "        \"\"\"Train the transformer model\"\"\"\n",
    "        self._initialize_weights(X.shape[1])\n",
    "        n_samples = X.shape[0]\n",
    "        best_loss = float('inf')\n",
    "        history = {'loss': [], 'accuracy': []}\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"üèãÔ∏è TRANSFORMER ENCODER TRAINING\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"  d_model={self.config.d_model}, heads={self.config.n_heads}, layers={self.config.n_layers}\")\n",
    "            print(f\"  Training samples: {n_samples}\")\n",
    "            print(\"-\"*60)\n",
    "        \n",
    "        for epoch in range(self.config.epochs):\n",
    "            predictions, _ = self.forward(X)\n",
    "            epsilon = 1e-7\n",
    "            loss = -np.mean(y * np.log(predictions + epsilon) + (1 - y) * np.log(1 - predictions + epsilon))\n",
    "            accuracy = np.mean((predictions > 0.5) == y) * 100\n",
    "            \n",
    "            gradient = (predictions - y).reshape(-1, 1)\n",
    "            self.weights['W_out'] -= self.config.learning_rate * np.dot(\n",
    "                np.dot(X, self.weights['W_embed']).T, gradient\n",
    "            ) / n_samples\n",
    "            \n",
    "            history['loss'].append(loss)\n",
    "            history['accuracy'].append(accuracy)\n",
    "            \n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "            \n",
    "            if verbose and (epoch + 1) % 20 == 0:\n",
    "                print(f\"  Epoch {epoch+1:3d}/{self.config.epochs} | Loss: {loss:.4f} | Accuracy: {accuracy:.1f}%\")\n",
    "        \n",
    "        self.is_trained = True\n",
    "        if verbose:\n",
    "            print(\"-\"*60)\n",
    "            print(f\"  ‚úÖ Training complete. Best loss: {best_loss:.4f}\")\n",
    "            print(\"=\"*60)\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict attrition probabilities\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before prediction\")\n",
    "        return self.forward(X)\n",
    "\n",
    "print(\"‚úÖ TransformerEncoder class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Step 3: SHAP Explainability Engine\n",
    "\n",
    "**SHAP (SHapley Additive exPlanations)** explains each prediction by assigning a contribution score to every feature.\n",
    "\n",
    "### Key Concept:\n",
    "Think of features as \"players\" in a cooperative game. The prediction is the \"payout.\" SHAP fairly distributes the payout based on each player's marginal contribution.\n",
    "\n",
    "### Formula:\n",
    "$$f(x) = \\phi_0 + \\sum_{i=1}^{M} \\phi_i(x)$$\n",
    "\n",
    "Where $\\phi_0$ is the base value and $\\phi_i$ is the Shapley value of feature $i$.\n",
    "\n",
    "### Shapley Value Calculation:\n",
    "$$\\phi_i(x) = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!(|N|-|S|-1)!}{|N|!} [f(S \\cup \\{i\\}) - f(S)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHAPExplainer:\n",
    "    \"\"\"\n",
    "    SHAP Explainability Engine\n",
    "    Calculates Shapley values for feature attribution.\n",
    "    Reference: Lundberg & Lee (2017)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: TransformerEncoder, feature_names: List[str]):\n",
    "        self.model = model\n",
    "        self.feature_names = feature_names\n",
    "        self.baseline = None\n",
    "        \n",
    "    def _get_baseline_prediction(self, X):\n",
    "        predictions, _ = self.model.predict(X)\n",
    "        return np.mean(predictions)\n",
    "    \n",
    "    def _calculate_shapley_value(self, x, feature_idx, X_background, n_samples=100):\n",
    "        \"\"\"Calculate Shapley value using Monte Carlo sampling\"\"\"\n",
    "        n_features = len(x)\n",
    "        shapley_values = []\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            perm = np.random.permutation(n_features)\n",
    "            feature_pos = np.where(perm == feature_idx)[0][0]\n",
    "            bg_idx = np.random.randint(len(X_background))\n",
    "            \n",
    "            x_with = X_background[bg_idx].copy()\n",
    "            x_without = X_background[bg_idx].copy()\n",
    "            \n",
    "            for j in range(feature_pos + 1):\n",
    "                x_with[perm[j]] = x[perm[j]]\n",
    "            for j in range(feature_pos):\n",
    "                x_without[perm[j]] = x[perm[j]]\n",
    "            \n",
    "            pred_with, _ = self.model.predict(x_with.reshape(1, -1))\n",
    "            pred_without, _ = self.model.predict(x_without.reshape(1, -1))\n",
    "            shapley_values.append(pred_with[0] - pred_without[0])\n",
    "        \n",
    "        return np.mean(shapley_values)\n",
    "    \n",
    "    def explain(self, X, X_background=None, max_samples=50):\n",
    "        \"\"\"Generate SHAP explanations\"\"\"\n",
    "        if X_background is None:\n",
    "            X_background = X\n",
    "            \n",
    "        self.baseline = self._get_baseline_prediction(X_background)\n",
    "        \n",
    "        # Limit samples for Colab performance\n",
    "        n_explain = min(X.shape[0], max_samples)\n",
    "        n_features = X.shape[1]\n",
    "        shap_values = np.zeros((n_explain, n_features))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üîç SHAP EXPLAINABILITY ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"  Calculating Shapley values for {n_explain} samples...\")\n",
    "        print(f\"  Features: {n_features}\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        for i in range(n_explain):\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  Processing sample {i+1}/{n_explain}...\")\n",
    "            for j in range(n_features):\n",
    "                shap_values[i, j] = self._calculate_shapley_value(X[i], j, X_background, n_samples=50)\n",
    "        \n",
    "        # Global feature importance\n",
    "        feature_importance = np.mean(np.abs(shap_values), axis=0)\n",
    "        feature_importance_normalized = feature_importance / (np.sum(feature_importance) + 1e-10)\n",
    "        \n",
    "        importance_ranking = []\n",
    "        for idx in np.argsort(feature_importance)[::-1]:\n",
    "            importance_ranking.append({\n",
    "                'feature': self.feature_names[idx],\n",
    "                'importance': float(feature_importance_normalized[idx]),\n",
    "                'mean_shap': float(np.mean(shap_values[:, idx])),\n",
    "                'direction': 'increases risk' if np.mean(shap_values[:, idx]) > 0 else 'decreases risk'\n",
    "            })\n",
    "        \n",
    "        print(\"-\"*60)\n",
    "        print(\"üìä Top Feature Importance (SHAP):\")\n",
    "        for i, feat in enumerate(importance_ranking[:10]):\n",
    "            direction = \"‚Üë risk\" if feat['direction'] == 'increases risk' else \"‚Üì risk\"\n",
    "            bar = \"‚ñà\" * int(feat['importance'] * 50)\n",
    "            print(f\"  {i+1:2d}. {feat['feature']:25s} {bar} {feat['importance']:.1%} ({direction})\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return {\n",
    "            'shap_values': shap_values,\n",
    "            'feature_importance': importance_ranking,\n",
    "            'base_value': self.baseline,\n",
    "            'feature_names': self.feature_names\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ SHAPExplainer class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Step 4: LDA Topic Modeling\n",
    "\n",
    "**LDA (Latent Dirichlet Allocation)** discovers hidden themes in employee reviews.\n",
    "\n",
    "### How it works:\n",
    "1. Each **document** (review) is a mixture of **topics**\n",
    "2. Each **topic** is a distribution over **words**\n",
    "3. **Gibbs Sampling** iteratively assigns words to topics\n",
    "\n",
    "### Sampling Formula:\n",
    "$$p(z_i = k | \\mathbf{z}_{-i}, \\mathbf{w}) \\propto (n_{d,k} + \\alpha) \\times \\frac{n_{k,w} + \\beta}{n_k + V\\beta}$$\n",
    "\n",
    "Where:\n",
    "- $n_{d,k}$ = words in document $d$ assigned to topic $k$\n",
    "- $n_{k,w}$ = times word $w$ assigned to topic $k$\n",
    "- $\\alpha$ = document-topic prior\n",
    "- $\\beta$ = topic-word prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDATopicModel:\n",
    "    \"\"\"\n",
    "    Latent Dirichlet Allocation using Collapsed Gibbs Sampling\n",
    "    Reference: Blei, Ng, Jordan (2003)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: LDAConfig = LDAConfig()):\n",
    "        self.config = config\n",
    "        self.vocabulary = {}\n",
    "        self.word_topic_counts = None\n",
    "        self.doc_topic_counts = None\n",
    "        self.topic_counts = None\n",
    "        self.topic_word_dist = None\n",
    "        \n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize and preprocess text\"\"\"\n",
    "        import re\n",
    "        text = text.lower()\n",
    "        words = re.findall(r'\\b[a-z]{3,}\\b', text)\n",
    "        stop_words = {\n",
    "            'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can',\n",
    "            'had', 'her', 'was', 'one', 'our', 'out', 'has', 'have', 'been',\n",
    "            'would', 'could', 'should', 'will', 'just', 'than', 'them', 'this',\n",
    "            'that', 'with', 'they', 'from', 'were', 'which', 'there', 'their',\n",
    "            'what', 'about', 'when', 'make', 'like', 'very', 'some', 'also',\n",
    "            'company', 'work', 'employee', 'employees', 'working', 'place'\n",
    "        }\n",
    "        return [w for w in words if w not in stop_words]\n",
    "    \n",
    "    def _build_vocabulary(self, documents):\n",
    "        word_counts = Counter()\n",
    "        for doc in documents:\n",
    "            word_counts.update(self._tokenize(doc))\n",
    "        vocab = {word: idx for idx, (word, count) in enumerate(word_counts.most_common()) if count >= 2}\n",
    "        doc_words = []\n",
    "        for doc in documents:\n",
    "            indices = [vocab[w] for w in self._tokenize(doc) if w in vocab]\n",
    "            doc_words.append(indices)\n",
    "        return vocab, doc_words\n",
    "    \n",
    "    def _analyze_sentiment(self, text: str) -> float:\n",
    "        positive = {'good', 'great', 'excellent', 'amazing', 'wonderful', 'best', 'happy', 'growth', 'opportunity', 'learning', 'support'}\n",
    "        negative = {'bad', 'poor', 'terrible', 'awful', 'worst', 'hate', 'stress', 'toxic', 'politics', 'underpaid', 'burnout', 'quit'}\n",
    "        tokens = set(self._tokenize(text))\n",
    "        pos = len(tokens & positive)\n",
    "        neg = len(tokens & negative)\n",
    "        return (pos - neg) / max(pos + neg, 1)\n",
    "    \n",
    "    def fit_transform(self, documents: List[str]) -> Dict:\n",
    "        \"\"\"Run LDA topic modeling\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìù LDA TOPIC MODELING\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"  Documents: {len(documents)}\")\n",
    "        print(f\"  Topics: {self.config.n_topics}\")\n",
    "        print(f\"  Iterations: {self.config.n_iterations}\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        self.vocabulary, doc_words = self._build_vocabulary(documents)\n",
    "        n_vocab = len(self.vocabulary)\n",
    "        print(f\"  Vocabulary size: {n_vocab}\")\n",
    "        \n",
    "        if n_vocab == 0:\n",
    "            return {'topics': [], 'doc_topics': np.array([])}\n",
    "        \n",
    "        # Initialize counts\n",
    "        n_topics = self.config.n_topics\n",
    "        self.word_topic_counts = np.zeros((n_vocab, n_topics)) + self.config.beta\n",
    "        self.doc_topic_counts = np.zeros((len(doc_words), n_topics)) + self.config.alpha\n",
    "        self.topic_counts = np.zeros(n_topics) + n_vocab * self.config.beta\n",
    "        \n",
    "        # Random initialization\n",
    "        topic_assignments = []\n",
    "        for d, words in enumerate(doc_words):\n",
    "            doc_topics = []\n",
    "            for w in words:\n",
    "                topic = np.random.randint(n_topics)\n",
    "                doc_topics.append(topic)\n",
    "                self.word_topic_counts[w, topic] += 1\n",
    "                self.doc_topic_counts[d, topic] += 1\n",
    "                self.topic_counts[topic] += 1\n",
    "            topic_assignments.append(doc_topics)\n",
    "        \n",
    "        # Gibbs Sampling\n",
    "        print(\"  Running Gibbs sampling...\")\n",
    "        for iteration in range(self.config.n_iterations):\n",
    "            if (iteration + 1) % 20 == 0:\n",
    "                print(f\"    Iteration {iteration+1}/{self.config.n_iterations}\")\n",
    "            for d, words in enumerate(doc_words):\n",
    "                for i, w in enumerate(words):\n",
    "                    old_topic = topic_assignments[d][i]\n",
    "                    self.word_topic_counts[w, old_topic] -= 1\n",
    "                    self.doc_topic_counts[d, old_topic] -= 1\n",
    "                    self.topic_counts[old_topic] -= 1\n",
    "                    \n",
    "                    topic_probs = self.doc_topic_counts[d] * self.word_topic_counts[w] / self.topic_counts\n",
    "                    topic_probs /= topic_probs.sum()\n",
    "                    \n",
    "                    new_topic = np.random.choice(n_topics, p=topic_probs)\n",
    "                    topic_assignments[d][i] = new_topic\n",
    "                    self.word_topic_counts[w, new_topic] += 1\n",
    "                    self.doc_topic_counts[d, new_topic] += 1\n",
    "                    self.topic_counts[new_topic] += 1\n",
    "        \n",
    "        self.topic_word_dist = self.word_topic_counts.T / self.word_topic_counts.sum(axis=0)\n",
    "        \n",
    "        # Extract topics\n",
    "        inv_vocab = {idx: word for word, idx in self.vocabulary.items()}\n",
    "        topics = []\n",
    "        \n",
    "        print(\"\\nüìã Extracted Topics:\")\n",
    "        print(\"-\"*60)\n",
    "        for k in range(n_topics):\n",
    "            word_indices = np.argsort(self.topic_word_dist[k])[::-1][:10]\n",
    "            keywords = [(inv_vocab[idx], float(self.topic_word_dist[k, idx])) for idx in word_indices]\n",
    "            \n",
    "            # Calculate sentiment\n",
    "            topic_docs = [documents[d] for d in range(len(documents)) if np.argmax(self.doc_topic_counts[d]) == k]\n",
    "            avg_sentiment = np.mean([self._analyze_sentiment(doc) for doc in topic_docs]) if topic_docs else 0\n",
    "            \n",
    "            topics.append({\n",
    "                'id': k,\n",
    "                'keywords': keywords,\n",
    "                'sentiment': avg_sentiment,\n",
    "                'label': 'Positive' if avg_sentiment > 0.1 else 'Negative' if avg_sentiment < -0.1 else 'Neutral',\n",
    "                'doc_count': len(topic_docs)\n",
    "            })\n",
    "            \n",
    "            kw_str = \", \".join([f\"{w} ({s:.3f})\" for w, s in keywords[:5]])\n",
    "            emoji = \"üòä\" if avg_sentiment > 0.1 else \"üòü\" if avg_sentiment < -0.1 else \"üòê\"\n",
    "            print(f\"  Topic {k+1}: {kw_str}\")\n",
    "            print(f\"           Sentiment: {avg_sentiment:.3f} {emoji} ({topics[-1]['label']})\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        return {'topics': topics, 'doc_topics': self.doc_topic_counts / self.doc_topic_counts.sum(axis=1, keepdims=True)}\n",
    "\n",
    "print(\"‚úÖ LDATopicModel class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Step 5: Risk Classification & Recommendations\n",
    "\n",
    "Converts raw probabilities into five actionable risk tiers with specific intervention strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_risk(probability: float) -> Dict:\n",
    "    \"\"\"Five-Tier Risk Classification\"\"\"\n",
    "    actions = {\n",
    "        'low': 'Continue monitoring. Maintain current engagement.',\n",
    "        'early_warning': 'Schedule check-in. Review recent feedback.',\n",
    "        'moderate': 'Initiate retention discussion. Consider role adjustments.',\n",
    "        'high': 'Urgent intervention. Discuss career development.',\n",
    "        'critical': 'Immediate action. Executive-level engagement.'\n",
    "    }\n",
    "    for level, (low, high) in RISK_THRESHOLDS.items():\n",
    "        if low <= probability < high:\n",
    "            return {'level': level, 'probability': probability, 'range': f\"{low:.0%}-{high:.0%}\", 'action': actions[level]}\n",
    "    return {'level': 'critical', 'probability': probability, 'range': '80%-100%', 'action': actions['critical']}\n",
    "\n",
    "print(\"‚úÖ Risk classification defined!\")\n",
    "print(\"\\nExample classifications:\")\n",
    "for p in [0.12, 0.35, 0.52, 0.71, 0.88]:\n",
    "    r = classify_risk(p)\n",
    "    print(f\"  p={p:.2f} ‚Üí {r['level'].upper():15s} | {r['action']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Step 6: Data Preprocessing\n",
    "\n",
    "The preprocessor handles multiple dataset formats (IBM HR, Kaggle, AmbitionBox) using column mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"Preprocess HR data for model training ‚Äî supports IBM, Kaggle, and AmbitionBox formats\"\"\"\n",
    "    \n",
    "    COLUMN_MAPPINGS = {\n",
    "        'satisfaction': ['JobSatisfaction', 'satisfaction_level', 'Rating', 'EnvironmentSatisfaction', 'Overall_rating', 'overall_rating'],\n",
    "        'tenure': ['YearsAtCompany', 'tenure', 'TotalWorkingYears', 'YearsInCurrentRole', 'time_spend_company'],\n",
    "        'overtime': ['OverTime', 'average_montly_hours', 'WorkLifeBalance', 'work_life_balance'],\n",
    "        'salary': ['MonthlyIncome', 'salary', 'DailyRate', 'HourlyRate', 'salary_and_benefits'],\n",
    "        'department': ['Department', 'department', 'JobRole'],\n",
    "        'promotion': ['YearsSinceLastPromotion', 'promotion_last_5years', 'career_growth'],\n",
    "        'projects': ['NumCompaniesWorked', 'number_project', 'skill_development'],\n",
    "        'attrition': ['Attrition', 'left', 'Status']\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_names = []\n",
    "        \n",
    "    def _find_column(self, df, candidates):\n",
    "        for col in candidates:\n",
    "            for df_col in df.columns:\n",
    "                if df_col.lower() == col.lower():\n",
    "                    return df_col\n",
    "        return None\n",
    "    \n",
    "    def fit_transform(self, df):\n",
    "        \"\"\"Preprocess dataframe and extract features\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìä DATA PREPROCESSING\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"  Original shape: {df.shape}\")\n",
    "        print(f\"  Columns: {list(df.columns[:10])}{'...' if len(df.columns) > 10 else ''}\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        features = []\n",
    "        feature_names = []\n",
    "        \n",
    "        for feature_type, candidates in self.COLUMN_MAPPINGS.items():\n",
    "            if feature_type == 'attrition':\n",
    "                continue\n",
    "            col = self._find_column(df, candidates)\n",
    "            if col is not None:\n",
    "                values = df[col].copy()\n",
    "                if values.dtype == 'object':\n",
    "                    if feature_type == 'overtime':\n",
    "                        values = values.map({'Yes': 1, 'No': 0, True: 1, False: 0}).fillna(0)\n",
    "                    elif feature_type == 'department':\n",
    "                        for dept in values.unique():\n",
    "                            if pd.notna(dept):\n",
    "                                features.append((values == dept).astype(float).values)\n",
    "                                feature_names.append(f'dept_{dept}')\n",
    "                        continue\n",
    "                    else:\n",
    "                        mapping = {v: i for i, v in enumerate(values.unique())}\n",
    "                        values = values.map(mapping).fillna(0)\n",
    "                \n",
    "                values = pd.to_numeric(values, errors='coerce').fillna(0)\n",
    "                if values.std() > 0:\n",
    "                    values = (values - values.mean()) / values.std()\n",
    "                features.append(values.values)\n",
    "                feature_names.append(feature_type)\n",
    "                print(f\"  ‚úì Added feature: {feature_type} (from column: {col})\")\n",
    "        \n",
    "        # Extract target\n",
    "        target_col = self._find_column(df, self.COLUMN_MAPPINGS['attrition'])\n",
    "        if target_col is not None:\n",
    "            y = df[target_col].copy()\n",
    "            if y.dtype == 'object':\n",
    "                y = y.map({'Yes': 1, 'No': 0, 'Left': 1, 'Stayed': 0}).fillna(0)\n",
    "            y = y.values.astype(float)\n",
    "            print(f\"  ‚úì Target: {target_col} (attrition rate: {y.mean():.1%})\")\n",
    "        else:\n",
    "            print(\"  ‚ö† No attrition column found ‚Äî generating synthetic target\")\n",
    "            y = np.random.binomial(1, 0.15, len(df))\n",
    "        \n",
    "        X = np.column_stack(features) if features else np.random.randn(len(df), 5)\n",
    "        self.feature_names = feature_names if feature_names else [f'feature_{i}' for i in range(X.shape[1])]\n",
    "        \n",
    "        print(f\"\\n  Processed shape: X={X.shape}, y={y.shape}\")\n",
    "        print(f\"  Features: {self.feature_names}\")\n",
    "        print(\"=\"*60)\n",
    "        return X, y, self.feature_names\n",
    "\n",
    "print(\"‚úÖ DataPreprocessor class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üè≠ Step 7: Generate Sample Data\n",
    "\n",
    "Generate a realistic HR dataset for demonstration. You can also upload your own CSV below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_data(n_samples=200):\n",
    "    \"\"\"Generate sample HR dataset for demonstration\"\"\"\n",
    "    np.random.seed(42)\n",
    "    departments = ['Sales', 'Engineering', 'HR', 'Marketing', 'Finance']\n",
    "    \n",
    "    data = {\n",
    "        'EmployeeID': range(1, n_samples + 1),\n",
    "        'Department': np.random.choice(departments, n_samples),\n",
    "        'JobSatisfaction': np.random.randint(1, 5, n_samples),\n",
    "        'YearsAtCompany': np.random.randint(0, 20, n_samples),\n",
    "        'MonthlyIncome': np.random.randint(3000, 15000, n_samples),\n",
    "        'OverTime': np.random.choice(['Yes', 'No'], n_samples, p=[0.3, 0.7]),\n",
    "        'WorkLifeBalance': np.random.randint(1, 5, n_samples),\n",
    "        'YearsSinceLastPromotion': np.random.randint(0, 10, n_samples),\n",
    "        'EnvironmentSatisfaction': np.random.randint(1, 5, n_samples),\n",
    "    }\n",
    "    \n",
    "    attrition_prob = (\n",
    "        (5 - data['JobSatisfaction']) * 0.1 +\n",
    "        (data['OverTime'] == 'Yes').astype(int) * 0.2 +\n",
    "        (data['YearsAtCompany'] < 2).astype(int) * 0.15 +\n",
    "        (data['WorkLifeBalance'] < 2).astype(int) * 0.1 +\n",
    "        np.random.uniform(0, 0.1, n_samples)\n",
    "    )\n",
    "    data['Attrition'] = ['Yes' if np.random.random() < p else 'No' for p in np.clip(attrition_prob, 0, 1)]\n",
    "    \n",
    "    issues = ['overtime is excessive', 'promotion opportunities are rare', 'workload is high',\n",
    "              'communication could improve', 'benefits need updating', 'stress levels are concerning']\n",
    "    templates = ['Good work environment but {}', 'I enjoy my work, however {}', 'Career growth is limited and {}']\n",
    "    data['Review'] = [np.random.choice(templates).format(np.random.choice(issues)) for _ in range(n_samples)]\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate and display sample data\n",
    "sample_df = generate_sample_data(200)\n",
    "print(f\"üìã Sample dataset generated: {sample_df.shape[0]} employees, {sample_df.shape[1]} columns\")\n",
    "print(f\"   Attrition rate: {(sample_df['Attrition'] == 'Yes').mean():.1%}\")\n",
    "print(f\"   Departments: {sample_df['Department'].unique().tolist()}\")\n",
    "sample_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì§ Step 8: Upload Your Own Dataset (Optional)\n",
    "\n",
    "Upload a CSV file to analyze your own HR data. Supported formats:\n",
    "- **IBM HR Analytics** (columns: Attrition, JobSatisfaction, Department, etc.)\n",
    "- **Kaggle HR Analytics** (columns: left, satisfaction_level, number_project, etc.)\n",
    "- **AmbitionBox Reviews** (columns: Overall_rating, work_life_balance, Likes, Dislikes, etc.)\n",
    "\n",
    "**If you skip this cell**, the analysis will use the sample data generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OPTION 1: Upload a CSV file ===\n",
    "# Uncomment the following lines to upload your own data:\n",
    "\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# filename = list(uploaded.keys())[0]\n",
    "# df = pd.read_csv(filename)\n",
    "# print(f\"\\n‚úÖ Loaded: {filename}\")\n",
    "# print(f\"   Shape: {df.shape}\")\n",
    "# print(f\"   Columns: {list(df.columns)}\")\n",
    "# df.head()\n",
    "\n",
    "# === OPTION 2: Use sample data (default) ===\n",
    "df = sample_df\n",
    "print(\"Using sample data (200 employees)\")\n",
    "print(\"üí° To use your own data, uncomment the upload section above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Step 9: Run the Complete Analysis Pipeline\n",
    "\n",
    "This executes the full pipeline:\n",
    "1. Data Preprocessing\n",
    "2. Transformer Training\n",
    "3. Prediction\n",
    "4. SHAP Analysis\n",
    "5. LDA Topic Modeling\n",
    "6. Risk Classification\n",
    "7. Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"   üß† EMPLOYEE ATTRITION PREDICTION SYSTEM\")\n",
    "print(\"   XAI-Powered Models for Managerial Decision-Making\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Step 1: Preprocess\n",
    "preprocessor = DataPreprocessor()\n",
    "X, y, feature_names = preprocessor.fit_transform(df)\n",
    "\n",
    "# Step 2: Train Transformer\n",
    "config = ModelConfig(epochs=100, n_layers=3)\n",
    "model = TransformerEncoder(config)\n",
    "history = model.fit(X, y)\n",
    "\n",
    "# Step 3: Predict\n",
    "predictions, attention_maps = model.predict(X)\n",
    "\n",
    "# Step 4: SHAP\n",
    "explainer = SHAPExplainer(model, feature_names)\n",
    "shap_results = explainer.explain(X, max_samples=min(50, len(X)))\n",
    "\n",
    "# Step 5: LDA (if text reviews available)\n",
    "lda_results = None\n",
    "text_col = None\n",
    "for col_name in ['Review', 'review', 'Likes', 'likes', 'Dislikes', 'dislikes']:\n",
    "    if col_name in df.columns:\n",
    "        text_col = col_name\n",
    "        break\n",
    "\n",
    "if text_col:\n",
    "    documents = df[text_col].dropna().tolist()\n",
    "    if len(documents) > 10:\n",
    "        lda_model = LDATopicModel(LDAConfig(n_topics=5, n_iterations=50))\n",
    "        lda_results = lda_model.fit_transform(documents)\n",
    "\n",
    "# Step 6: Risk Classification\n",
    "risk_classifications = [classify_risk(p) for p in predictions]\n",
    "risk_counts = Counter(r['level'] for r in risk_classifications)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"   üìä ANALYSIS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Total Employees: {len(df)}\")\n",
    "print(f\"  Historical Attrition Rate: {y.mean():.1%}\")\n",
    "print(f\"  Model Accuracy: {np.mean((predictions > 0.5) == y) * 100:.1f}%\")\n",
    "high_risk = risk_counts.get('high', 0) + risk_counts.get('critical', 0)\n",
    "print(f\"  High/Critical Risk: {high_risk} employees\")\n",
    "print(f\"\\n  Risk Distribution:\")\n",
    "for level in ['low', 'early_warning', 'moderate', 'high', 'critical']:\n",
    "    count = risk_counts.get(level, 0)\n",
    "    pct = count / len(predictions) * 100\n",
    "    bar = \"‚ñà\" * int(pct / 2) + \"‚ñë\" * (50 - int(pct / 2))\n",
    "    print(f\"    {level.upper():15s}: {bar} {count:4d} ({pct:.1f}%)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìà Step 10: Visualizations\n",
    "\n",
    "Professional charts showing the analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Training History ===\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history['loss'], color='#ef4444', linewidth=2)\n",
    "axes[0].set_title('Training Loss (Binary Cross-Entropy)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history['accuracy'], color='#22c55e', linewidth=2)\n",
    "axes[1].set_title('Training Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"üìà Training curves show model convergence over epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. Risk Distribution (Donut Chart) ===\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "labels = []\n",
    "sizes = []\n",
    "colors = []\n",
    "explode_vals = []\n",
    "\n",
    "for level in ['low', 'early_warning', 'moderate', 'high', 'critical']:\n",
    "    count = risk_counts.get(level, 0)\n",
    "    if count > 0:\n",
    "        labels.append(f\"{level.replace('_', ' ').title()}\\n({count})\")\n",
    "        sizes.append(count)\n",
    "        colors.append(RISK_COLORS[level])\n",
    "        explode_vals.append(0.05 if level in ['high', 'critical'] else 0)\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    sizes, labels=labels, colors=colors, explode=explode_vals,\n",
    "    autopct='%1.1f%%', pctdistance=0.75,\n",
    "    wedgeprops={'width': 0.5, 'edgecolor': 'white', 'linewidth': 2},\n",
    "    textprops={'fontsize': 11}\n",
    ")\n",
    "for autotext in autotexts:\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "ax.set_title('Five-Tier Risk Distribution', fontsize=16, fontweight='bold', pad=20)\n",
    "centre = plt.Circle((0, 0), 0.3, fc='white')\n",
    "ax.add_patch(centre)\n",
    "ax.text(0, 0, f'{len(predictions)}\\nTotal', ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"üç© Donut chart shows the distribution across all five risk tiers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. Feature Importance (SHAP) ===\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "top_features = shap_results['feature_importance'][:10]\n",
    "feat_names = [f['feature'] for f in top_features][::-1]\n",
    "feat_values = [f['importance'] for f in top_features][::-1]\n",
    "feat_colors = ['#ef4444' if f['direction'] == 'increases risk' else '#22c55e' for f in top_features][::-1]\n",
    "\n",
    "bars = ax.barh(feat_names, feat_values, color=feat_colors, edgecolor='white', linewidth=1, height=0.6)\n",
    "ax.set_title('Global Feature Importance (SHAP Values)', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Mean |SHAP Value| (Normalized Importance)', fontsize=12)\n",
    "\n",
    "for bar, val in zip(bars, feat_values):\n",
    "    ax.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height()/2,\n",
    "            f'{val:.1%}', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#ef4444', label='Increases Attrition Risk'),\n",
    "    Patch(facecolor='#22c55e', label='Decreases Attrition Risk')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='lower right', fontsize=11)\n",
    "ax.grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"üìä SHAP bar chart shows which features drive attrition predictions\")\n",
    "print(\"   üî¥ Red = increases risk | üü¢ Green = decreases risk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. Risk Score Distribution (Histogram with zones) ===\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Background risk zones\n",
    "zone_colors = ['#dcfce7', '#dbeafe', '#fef3c7', '#fed7aa', '#fecaca']\n",
    "zone_labels = ['Low', 'Early\\nWarning', 'Moderate', 'High', 'Critical']\n",
    "boundaries = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "for i in range(5):\n",
    "    ax.axvspan(boundaries[i], boundaries[i+1], alpha=0.4, color=zone_colors[i])\n",
    "    ax.text((boundaries[i] + boundaries[i+1]) / 2, ax.get_ylim()[1] * 0.02,\n",
    "            zone_labels[i], ha='center', fontsize=9, color='gray', style='italic')\n",
    "\n",
    "ax.hist(predictions, bins=30, color='#3b82f6', edgecolor='white', alpha=0.8, linewidth=1)\n",
    "ax.set_title('Attrition Risk Score Distribution', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Risk Score (Probability)', fontsize=12)\n",
    "ax.set_ylabel('Number of Employees', fontsize=12)\n",
    "ax.set_xlim(0, 1)\n",
    "\n",
    "# Re-add zone labels after histogram sets y-limits\n",
    "ylim = ax.get_ylim()\n",
    "for i in range(5):\n",
    "    ax.text((boundaries[i] + boundaries[i+1]) / 2, ylim[1] * 0.95,\n",
    "            zone_labels[i], ha='center', fontsize=9, color='gray', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"üìä Histogram shows how risk scores are distributed across the workforce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. Individual Employee Explanation (SHAP Waterfall) ===\n",
    "# Show SHAP explanation for the highest-risk employee\n",
    "highest_risk_idx = np.argmax(predictions)\n",
    "employee_shap = shap_results['shap_values'][min(highest_risk_idx, len(shap_results['shap_values'])-1)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "sorted_idx = np.argsort(np.abs(employee_shap))[::-1][:8]\n",
    "feat_labels = [feature_names[i] if i < len(feature_names) else f'Feature {i}' for i in sorted_idx]\n",
    "shap_vals = [employee_shap[i] for i in sorted_idx]\n",
    "bar_colors = ['#ef4444' if v > 0 else '#22c55e' for v in shap_vals]\n",
    "\n",
    "bars = ax.barh(feat_labels[::-1], [v for v in shap_vals[::-1]], \n",
    "               color=bar_colors[::-1], edgecolor='white', height=0.6)\n",
    "\n",
    "ax.set_title(f'SHAP Explanation ‚Äî Employee #{highest_risk_idx+1} (Risk: {predictions[highest_risk_idx]:.1%})',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('SHAP Value (Impact on Risk)', fontsize=12)\n",
    "ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"üîç Individual explanation for the highest-risk employee\")\n",
    "print(f\"   Risk: {risk_classifications[highest_risk_idx]['level'].upper()} ({predictions[highest_risk_idx]:.1%})\")\n",
    "print(f\"   Action: {risk_classifications[highest_risk_idx]['action']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6. Department Analysis ===\n",
    "if 'Department' in df.columns:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    dept_risks = {}\n",
    "    for dept in df['Department'].unique():\n",
    "        dept_mask = (df['Department'] == dept).values\n",
    "        dept_preds = predictions[dept_mask]\n",
    "        dept_risks[dept] = {\n",
    "            'total': len(dept_preds),\n",
    "            'at_risk': sum(1 for p in dept_preds if p > 0.5),\n",
    "            'avg_risk': np.mean(dept_preds)\n",
    "        }\n",
    "    \n",
    "    depts = sorted(dept_risks.keys(), key=lambda d: dept_risks[d]['avg_risk'], reverse=True)\n",
    "    x = np.arange(len(depts))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x - width/2, [dept_risks[d]['total'] for d in depts], width, \n",
    "           label='Total', color='#3b82f6', alpha=0.7)\n",
    "    ax.bar(x + width/2, [dept_risks[d]['at_risk'] for d in depts], width,\n",
    "           label='At Risk (>50%)', color='#ef4444', alpha=0.7)\n",
    "    \n",
    "    ax.set_title('Department-wise Attrition Risk', fontsize=16, fontweight='bold')\n",
    "    ax.set_ylabel('Number of Employees', fontsize=12)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(depts, fontsize=11)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"üìä Department analysis shows which teams have highest attrition risk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üíæ Step 11: Save Results\n",
    "\n",
    "Export the complete analysis results to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results\n",
    "results = {\n",
    "    'summary': {\n",
    "        'total_employees': len(df),\n",
    "        'attrition_rate': float(y.mean()),\n",
    "        'model_accuracy': float(np.mean((predictions > 0.5) == y)),\n",
    "        'high_risk_count': int(risk_counts.get('high', 0) + risk_counts.get('critical', 0)),\n",
    "    },\n",
    "    'risk_distribution': {level: int(risk_counts.get(level, 0)) for level in RISK_THRESHOLDS},\n",
    "    'feature_importance': shap_results['feature_importance'][:10],\n",
    "    'model_config': {\n",
    "        'd_model': config.d_model,\n",
    "        'n_heads': config.n_heads,\n",
    "        'n_layers': config.n_layers,\n",
    "        'epochs': config.epochs,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "with open('attrition_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "print(\"‚úÖ Results saved to 'attrition_results.json'\")\n",
    "print(\"\\nüìã Results Summary:\")\n",
    "print(json.dumps(results['summary'], indent=2))\n",
    "print(\"\\nüìä Risk Distribution:\")\n",
    "print(json.dumps(results['risk_distribution'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results (uncomment to download)\n",
    "# from google.colab import files\n",
    "# files.download('attrition_results.json')\n",
    "print(\"üí° Uncomment the lines above to download the results file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Key Terminology Reference\n",
    "\n",
    "| Term | Definition |\n",
    "|------|------------|\n",
    "| **Transformer Encoder** | Neural network using self-attention to learn feature relationships |\n",
    "| **Self-Attention** | Mechanism that weighs relationships between ALL input features simultaneously |\n",
    "| **SHAP** | Shapley Additive Explanations ‚Äî fairly distributes prediction credit to features |\n",
    "| **LDA** | Latent Dirichlet Allocation ‚Äî discovers hidden topics in text data |\n",
    "| **Gibbs Sampling** | Iterative algorithm used by LDA to assign words to topics |\n",
    "| **Five-Tier Risk** | Low, Early Warning, Moderate, High, Critical classification system |\n",
    "| **Calibration** | Blending model output with domain heuristics for realistic probabilities |\n",
    "| **Feature Attribution** | Assigning importance scores to each input variable |\n",
    "| **SELU** | Self-Normalizing Exponential Linear Unit activation function |\n",
    "| **Layer Normalization** | Technique to stabilize training by normalizing activations |\n",
    "| **Binary Cross-Entropy** | Loss function for measuring binary classification error |\n",
    "| **Xavier Initialization** | Weight initialization strategy preventing gradient issues |\n",
    "\n",
    "---\n",
    "\n",
    "## üìö References\n",
    "\n",
    "1. Vaswani et al. (2017). *\"Attention Is All You Need.\"* NeurIPS.\n",
    "2. Lundberg & Lee (2017). *\"A Unified Approach to Interpreting Model Predictions.\"* NeurIPS.\n",
    "3. Blei, Ng, Jordan (2003). *\"Latent Dirichlet Allocation.\"* JMLR.\n",
    "4. Baydili & Tasci (2025). *\"Predicting Employee Attrition: XAI-Powered Models.\"* Systems 13, 583.\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook created for project review demonstration*"
   ]
  }
 ]
}